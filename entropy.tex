
%beginning of problems

\section*{Entropy}
Entropy is a measure of the uncertainty of a random variable. It quantifies the amount of information required on average to describe the outcome of the random variable. The entropy of a random variable is measured in bits.

\begin{definition}[entropy]
  Let $X$ be a discrete random variable with alphabet $\mathcal{X} = \text{Im}(X)$ and probability mass function $p(x) = \text{Pr} \{ X = x \}, x \in \mathcal{H}$. The \textbf{entropy} of $X$ is defined by
  \begin{align*}
    H(X) = - \sum_{x \in \mathcal{X}} p(x) \log_{2} p(x)
  \end{align*}

\end{definition}


\section*{Conditional entropy}

Conditional entropy is the entropy of a random variable conditioned on knowing the outcome of a different random variable. It quantifies the amount of information required on average to describe the outcome of a random variable $Y$ given that we know the value of another random variable $Y$.

\begin{definition}(conditional entropy)
Let $(X,Y)$ be a pair of discrete random variables with alphabets $\mathcal{X}$ and $\mathcal{Y}$ respectively and joint probability mass function $p_{X,Y}(x,y) = \text{Pr} \{ X = x, Y = y \}$. The conditional entropy of $Y$ given $X$ is defined by

\begin{align*}
  & H(Y|X) = \sum_{x \in \mathcal{X}} p(x)H(Y|X = x) \\
  & = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log_{2} p(y|x)
\end{align*}

\end{definition}

\section*{Previous uses of entropy in caching}

\section*{Predictability vs cacheability}

\section*{Trace Sources}
